 [Encoder]
hidden_size = 512
max_sequence_length = 100
max_sequence_sent_length = 10
num_layers = 2
cell_type = LSTM
is_bidir = True
use_self_attention = False
use_gated_self_attention = False
self_attention_heads = 0
use_property_attention = False
num_property_values = 0
use_js = False
use_coattention = True
use_gated_fusion = False
use_gated_fusion_encoder = False
use_gated_fusion_both=False

[Question_encoder]
hidden_size = 128
num_layers = 1
use_question_encoder = False
use_gate_policy  = False
use_rl_on_gate = False
cell_type= LSTM
gate_loss_lambda = 1.0
use_word_level_gate = False
use_gated_context = False
word_concat= False

[Decoder]
hidden_size = 512
num_layers = 1
cell_type = LSTM
decoder_type = copy
max_sequence_length = 20
max_decoder_steps = 20
use_distraction = False
min_dec_steps = 5
use_coverage = True
include_prop_weights = False
max_prop_steps = 20 
use_three_gates_switch = False
is_memnet = True 
is_memnet_concat = False 
no_prop_copy = True
reduce_length=False
use_dropout_on_masked_weights=False
masked_weights_keep_prob = 1
use_diff_out_proj = False
use_single_layer = True
use_attention_different = False
use_word_embed_outputproj = False
use_concat= False
use_attn_product = False

[Ablation_D1_D2_path]
input_to_cell_2 = True
input_to_attention_on_encoder = False
input_to_comb_projection = True

[Query]
max_sequence_length = 20
num_layers = 2
hidden_size =512
use_position = True
cell_type = LSTM
use_query = True

[Attention]
num_heads = 2
hidden_size = 400
attention_type = Bahadanau

[GCN]
num_gcns = 1
num_layers = 3
hidden_units_per_layer = [1024,1024,1024]
use_passage_gcn = False
use_query_gcn = False

[Embedding]
word_embedding_size = 300
char_in_embed_size = 20
char_out_embed_size = 100
word_vocab_length = 30000
char_vocab_length = 1000
max_word_length = 20
embedding_dir = embeddings/
out_channel_dims = 100
char_filter_heights = 5
char_out_channel_dims = 100
max_sent_size = 50
use_char_embed = True
use_positional_embeddings=True
position_embeddings_dims = 100

[Hyperparams]
initializer = xavier
batch_size = 32
optimizer = Adam
learning_rate = 0.0004
gradient_clip = 1
max_epochs = 10
early_stop = 30
feed_previous = 25
feed_true = 0
keep_prob = 0.8
add_switch_loss = False
switch_multiplier = 2.0
use_only_rl = False
use_rl_plus_entropy = False
rl_lambda_value = 0.9984
random_seed = 1357
use_qbleu = False
use_pos_decoder = False
rl_plus_crossent = False
epoch_before_rl = 0
use_dynamic_score = False
coverage_lambda = 0
coverage_property_lambda = 0
js_lambda = 0
use_same_prop_grammar = True
use_greedy_policy = False
l2_lambda = 0
use_batch_norm = False
use_reverse_rl = False
use_only_reverse_rl = False

[BeamSearch]
beam_size =5
batch_size=40

[Aux_Loss]
prev_decoder_prop = True
prev_decoder_prop_lambda = 1
use_loss_question_label = False
question_label_lambda = 0
use_prop_label = False
prop_label_lambda = 0
use_quora_loss = False
double_stocastic_attention = False
double_stocastic_attention_lambda = 0
last_decoder_loss_lambda = 1
prop_indices_loss = False
prop_indices_lambda = 0
stop_word_weight = 1
mutual_information = False
mutual_information_lambda = 0

[Log]
eval_after_steps = 500
print_frequency = 500
print_summaries = True
output_dir = experiments/sentencelevel/vanilla_twodecoder/

input_dir = ./

[POS]
pos_embed_size = 20
use_pos_loss = False
num_pos_words = 500
lambda_value = 0.2
